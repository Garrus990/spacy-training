{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1233b4-7792-47c9-affb-4b7f5d290a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f6826-94bc-4865-810d-02f9eadc4d2b",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29801209-7900-4ef1-86cc-18545aae6959",
   "metadata": {},
   "source": [
    "### The nlp object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24c91d-6368-4a20-bca9-c57bb16bb76b",
   "metadata": {},
   "source": [
    "- contains the processing pipeline\n",
    "- includes language-specific rules for tokenization etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428ebac3-3958-4b84-a67a-e5a56cbdb1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank English nlp object\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393440d5-4ec0-4be0-8661-539bfd2b6541",
   "metadata": {},
   "source": [
    "### The Doc object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e9a18-4347-4e6a-a90e-64b7b2e94e6f",
   "metadata": {},
   "source": [
    "Created by processing a string of text with the nlp object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f576d4da-5610-42fc-8518-95627ec2ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a2f6a-d954-4f42-9632-7373f190cb35",
   "metadata": {},
   "source": [
    "### The Token object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583ed40a-9cda-4247-bd5c-00ee1064a605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "# Index into the Doc to get a single Token\n",
    "token = doc[1]\n",
    "\n",
    "# Get the token text via the .text attribute\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92fa7b1-7aaa-4c01-9539-7b54f42cef6b",
   "metadata": {},
   "source": [
    "### The Span object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351147c-e2e4-46a9-9786-c17d602e35f2",
   "metadata": {},
   "source": [
    "A slice from the Doc is a Span object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e42cb20-8850-416f-9c6d-e4a655a78f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "span = doc[1:3]\n",
    "\n",
    "# Get the span text via the .text attribute\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8cca46b-81aa-4043-80e1-d705b6fef6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span=world!\n"
     ]
    }
   ],
   "source": [
    "print(f\"{span=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b51aef-f43c-4226-bc9f-a7dfac983aa9",
   "metadata": {},
   "source": [
    "### Lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96531070-6357-482f-9cbb-f0181fd092b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:    [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Text:     ['It', 'costs', '$', '5', '(', 'five', 'dollars', ')', '.']\n",
      "is_alpha: [True, True, False, False, False, True, True, False, False]\n",
      "is_punct: [False, False, False, False, True, False, False, True, True]\n",
      "like_num: [False, False, False, True, False, True, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It costs $5 (five dollars).\")\n",
    "\n",
    "print(\"Index:   \", [token.i for token in doc])\n",
    "print(\"Text:    \", [token.text for token in doc])\n",
    "\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc])\n",
    "print(\"like_num:\", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a60d5e-aa43-4928-850d-fc426c394926",
   "metadata": {},
   "source": [
    "# Trained pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910391a9-34ae-4a96-a4fc-e260f79d5501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to get trained pipelines for English\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a2f2f-5bbf-4464-9906-83998e827d7e",
   "metadata": {},
   "source": [
    "What is comprised?\n",
    "\n",
    "- Binary weights\n",
    "- Vocabulary\n",
    "- Meta information\n",
    "- Configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c81ef310-ea27-4603-959c-60eeb94bbfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd5af7-a3b8-43bb-86db-5b48fdb6b724",
   "metadata": {},
   "source": [
    "### Predicting Part-of-speech Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0321e20-b415-459b-a90d-22c06b80032c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON pronoun\n",
      "ate VERB verb\n",
      "the DET determiner\n",
      "pizza NOUN noun\n",
      ". PUNCT punctuation\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"She ate the pizza.\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_, spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e32aa-831f-4679-9bd7-a189838156fe",
   "metadata": {},
   "source": [
    "### Predicting Syntactic Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4d96fa6-c71c-4526-8b70-39ea364519aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate nominal subject\n",
      "ate VERB ROOT ate root\n",
      "the DET det pizza determiner\n",
      "pizza NOUN dobj ate direct object\n",
      ". PUNCT punct ate punctuation\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text, spacy.explain(token.dep_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7583b-283b-4096-b070-a51bb3f1dd06",
   "metadata": {},
   "source": [
    "### Predicting Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82b7a47a-d444-46e1-982c-ed27838f1f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG Companies, agencies, institutions, etc.\n",
      "U.K. GPE Countries, cities, states\n",
      "$1 billion MONEY Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb8d05-8fb7-4dce-850e-a7ff521f1f7b",
   "metadata": {},
   "source": [
    "## Matchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00df4f55-6c30-4571-9b85-a1056936b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a pipeline and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d703c86-2243-434b-a6ad-387ba41bbf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8312786-0121-414f-aae1-eee7175e58dc",
   "metadata": {},
   "source": [
    "### Matching lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "248a6b5c-fc8c-4787-b7ca-9bc2351a5df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"world_cup_pattern\", [pattern])\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ad7c7-08cf-4f71-9c42-88ca6c5e2e2a",
   "metadata": {},
   "source": [
    "### Matching other token attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "235cd847-48ec-410b-b42d-3a131a2e93e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"loving_something_pattern\", [pattern])\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a345b88-c0a5-4001-9ba5-7eb7ebe9e8ae",
   "metadata": {},
   "source": [
    "### Using operators and quantifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd59d8-e75f-4598-8425-60804767560a",
   "metadata": {},
   "source": [
    "- {\"OP\": \"!\"}: \tNegation: match 0 times\n",
    "- {\"OP\": \"?\"}: \tOptional: match 0 or 1 times\n",
    "- {\"OP\": \"+\"}: \tMatch 1 or more times\n",
    "- {\"OP\": \"*\"}: \tMatch 0 or more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98375200-9ef0-4139-8d28-91e08fa1b666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"buying_something_possibly_with_determiner\", [pattern])\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
